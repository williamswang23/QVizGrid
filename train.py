# Copyright (c) 2025, Williams.Wang. All rights reserved. Use restricted under LICENSE terms.

"""
Q-Learning GridWorld ä¸»è®­ç»ƒè„šæœ¬
å®æ—¶å¯è§†åŒ– Q-table å’Œ V-table çš„æ¼”åŒ–è¿‡ç¨‹
"""

import yaml
import numpy as np
from typing import Dict, List, Tuple, Any
import os
from collections import deque

from rich.live import Live
from rich.console import Console

from envs.gridworld import GridWorld
from agent.q_learning import QLearningAgent
from viz.live_view import render_layout, save_q_table_snapshot, create_gif_from_frames, cleanup_frames


def load_config(config_path: str = "config.yaml") -> Dict[str, Any]:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é…ç½®å­—å…¸
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def check_convergence(agent: QLearningAgent, episode_rewards: deque, 
                     config: Dict[str, Any]) -> Tuple[bool, str]:
    """
    æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ”¶æ•›æ¡ä»¶
    
    Args:
        agent: Q-Learningæ™ºèƒ½ä½“
        episode_rewards: æœ€è¿‘episodesçš„å¥–åŠ±å†å²
        config: è®­ç»ƒé…ç½®
        
    Returns:
        (æ˜¯å¦æ”¶æ•›, æ”¶æ•›åŸå› )
    """
    # æ£€æŸ¥Qå€¼å˜åŒ–æ”¶æ•›
    max_q_change = agent.get_max_q_change()
    if max_q_change != float('inf') and max_q_change < config['convergence_tol']:
        return True, f"Q-value convergence: max change {max_q_change:.6f} < {config['convergence_tol']}"
    
    # æ£€æŸ¥å¹³å‡å›æŠ¥ç¨³å®šï¼ˆéœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®ï¼‰
    if len(episode_rewards) >= 100:
        recent_rewards = list(episode_rewards)
        recent_mean = np.mean(recent_rewards[-50:])
        earlier_mean = np.mean(recent_rewards[-100:-50])
        
        if abs(recent_mean - earlier_mean) < 1e-3:
            return True, f"Reward stability: recent={recent_mean:.4f}, earlier={earlier_mean:.4f}"
    
    return False, ""


def run_episode(env: GridWorld, agent: QLearningAgent) -> Tuple[float, int]:
    """
    è¿è¡Œä¸€ä¸ªå®Œæ•´çš„episode
    
    Args:
        env: GridWorldç¯å¢ƒ
        agent: Q-Learningæ™ºèƒ½ä½“
        
    Returns:
        (æ€»å¥–åŠ±, æ­¥æ•°)
    """
    # é‡ç½®ç¯å¢ƒ
    state_pos = env.reset()
    state_index = env.get_state_index(state_pos)
    
    total_reward = 0.0
    steps = 0
    max_steps = 200  # é˜²æ­¢æ— é™å¾ªç¯
    
    while steps < max_steps:
        # é€‰æ‹©åŠ¨ä½œ
        action = agent.choose_action(state_index)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_state_pos, reward, done, info = env.step(action)
        next_state_index = env.get_state_index(next_state_pos)
        
        # æ›´æ–°Q-table
        q_change = agent.update(state_index, action, reward, next_state_index, done)
        
        # ç´¯è®¡å¥–åŠ±å’Œæ­¥æ•°
        total_reward += reward
        steps += 1
        
        # æ›´æ–°çŠ¶æ€
        state_index = next_state_index
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        if done:
            break
    
    return total_reward, steps


def print_episode_info(episode: int, total_reward: float, steps: int, 
                      agent: QLearningAgent, episode_rewards: deque) -> None:
    """
    æ‰“å°episodeä¿¡æ¯
    
    Args:
        episode: episodeç¼–å·
        total_reward: æ€»å¥–åŠ±
        steps: æ­¥æ•°
        agent: æ™ºèƒ½ä½“
        episode_rewards: å¥–åŠ±å†å²
    """
    if len(episode_rewards) >= 100:
        avg_reward = np.mean(list(episode_rewards)[-100:])
        print(f"Episode {episode:4d} | Reward: {total_reward:6.2f} | Steps: {steps:3d} | "
              f"Avg100: {avg_reward:6.2f} | Îµ: {agent.epsilon:.4f}")
    else:
        print(f"Episode {episode:4d} | Reward: {total_reward:6.2f} | Steps: {steps:3d} | "
              f"Îµ: {agent.epsilon:.4f}")


def main():
    """
    ä¸»è®­ç»ƒå‡½æ•°
    """
    console = Console()
    console.print("ğŸš€ Starting Q-Learning GridWorld Training...", style="bold green")
    
    # åŠ è½½é…ç½®
    config = load_config()
    console.print("ğŸ“‹ Configuration loaded from config.yaml")
    
    # åˆ›å»ºç¯å¢ƒ
    env = GridWorld(config['env'])
    console.print(f"ğŸŒ GridWorld environment created: {env.grid_size}x{env.grid_size} grid")
    console.print(f"   Start: {env.start_pos}, Goal: {env.goal_pos}")
    console.print(f"   Obstacles: {env.holes}")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = QLearningAgent(config['agent'], env.num_states, env.num_actions)
    console.print("ğŸ¤– Q-Learning agent created")
    console.print(f"   Learning rate: {agent.alpha}")
    console.print(f"   Discount factor: {agent.gamma}")
    console.print(f"   Epsilon: {agent.epsilon_start} â†’ {agent.epsilon_end}")
    
    # è®­ç»ƒé…ç½®
    train_config = config['train']
    max_episodes = train_config['max_episodes']
    render_interval = train_config.get('render_interval', 10)
    use_visualization = train_config.get('use_visualization', True)
    
    # GIFå½•åˆ¶é…ç½®
    snapshot_interval = train_config.get('snapshot_interval', 100)
    create_gif = train_config.get('create_gif', True)
    frames_dir = "outputs/frames"
    gif_output_path = "outputs/q_evolution.gif"
    
    # è®­ç»ƒç»Ÿè®¡
    episode_rewards = deque(maxlen=1000)
    convergence_patience = train_config.get('convergence_patience', 20)
    convergence_count = 0
    
    console.print(f"\nğŸ¯ Training Configuration:")
    console.print(f"   Max episodes: {max_episodes}")
    console.print(f"   Render interval: {render_interval}")
    console.print(f"   Use visualization: {use_visualization}")
    console.print(f"   Snapshot interval: {snapshot_interval}")
    console.print(f"   Create GIF: {create_gif}")
    console.print(f"   Convergence tolerance: {train_config['convergence_tol']}")
    console.print(f"   Convergence patience: {convergence_patience}")
    
    # æ¸…ç†æ—§çš„å¸§æ–‡ä»¶
    if create_gif:
        cleanup_frames(frames_dir)
    
    if use_visualization:
        console.print("\nğŸ¬ Starting live visualization...", style="bold cyan")
        
        # åˆå§‹åŒ–å¯è§†åŒ–
        v_table = agent.get_v_table()
        policy = agent.get_policy()
        initial_stats = {
            'episode': 0,
            'max_episodes': max_episodes,
            'epsilon': agent.epsilon,
            'total_reward': 0.0,
            'avg_reward': 0.0,
            'max_q_change': 0.0,
            'steps': 0
        }
        
        # ä½¿ç”¨Rich Liveè¿›è¡Œå®æ—¶å¯è§†åŒ–
        with Live(render_layout(agent.q_table, v_table, policy, env, initial_stats), 
                  refresh_per_second=2, console=console) as live:
            
            # ä¸»è®­ç»ƒå¾ªç¯
            for episode in range(1, max_episodes + 1):
                # ä¿å­˜Q-tableå¿«ç…§ç”¨äºæ”¶æ•›æ£€æŸ¥
                if episode > 1:
                    agent.save_q_table_snapshot()
                
                # è¿è¡Œä¸€ä¸ªepisode
                total_reward, steps = run_episode(env, agent)
                episode_rewards.append(total_reward)
                
                # è¡°å‡epsilon
                agent.decay_epsilon()
                
                # æ›´æ–°å¯è§†åŒ–
                if episode % render_interval == 0 or episode <= 10:
                    v_table = agent.get_v_table()
                    policy = agent.get_policy()
                    max_q_change = agent.get_max_q_change()
                    avg_reward = np.mean(list(episode_rewards)[-100:]) if len(episode_rewards) >= 100 else np.mean(list(episode_rewards))
                    
                    stats = {
                        'episode': episode,
                        'max_episodes': max_episodes,
                        'epsilon': agent.epsilon,
                        'total_reward': total_reward,
                        'avg_reward': avg_reward,
                        'max_q_change': max_q_change,
                        'steps': steps
                    }
                    
                    live.update(render_layout(agent.q_table, v_table, policy, env, stats))
                
                # ä¿å­˜GIFå¸§
                if create_gif and episode % snapshot_interval == 0:
                    v_table = agent.get_v_table()
                    policy = agent.get_policy()
                    max_q_change = agent.get_max_q_change()
                    avg_reward = np.mean(list(episode_rewards)[-100:]) if len(episode_rewards) >= 100 else np.mean(list(episode_rewards))
                    
                    stats = {
                        'episode': episode,
                        'max_episodes': max_episodes,
                        'epsilon': agent.epsilon,
                        'total_reward': total_reward,
                        'avg_reward': avg_reward,
                        'max_q_change': max_q_change,
                        'steps': steps
                    }
                    
                    frame_path = os.path.join(frames_dir, f"frame_{episode:04d}.png")
                    save_q_table_snapshot(agent.q_table, v_table, policy, env, stats, frame_path)
                
                # æ£€æŸ¥æ”¶æ•›æ¡ä»¶
                if episode > 50:
                    converged, reason = check_convergence(agent, episode_rewards, train_config)
                    if converged:
                        convergence_count += 1
                        if convergence_count >= convergence_patience:
                            console.print(f"\nğŸ‰ Training converged after {episode} episodes!", style="bold green")
                            console.print(f"   Reason: {reason}")
                            break
                    else:
                        convergence_count = 0
    else:
        console.print("\nğŸ“Š Training without visualization...")
        console.print("-" * 80)
        
        # ä¸»è®­ç»ƒå¾ªç¯ï¼ˆæ— å¯è§†åŒ–ï¼‰
        for episode in range(1, max_episodes + 1):
            # ä¿å­˜Q-tableå¿«ç…§ç”¨äºæ”¶æ•›æ£€æŸ¥
            if episode > 1:
                agent.save_q_table_snapshot()
            
            # è¿è¡Œä¸€ä¸ªepisode
            total_reward, steps = run_episode(env, agent)
            episode_rewards.append(total_reward)
            
            # è¡°å‡epsilon
            agent.decay_epsilon()
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯
            if episode % render_interval == 0 or episode <= 10:
                print_episode_info(episode, total_reward, steps, agent, episode_rewards)
            
            # ä¿å­˜GIFå¸§ï¼ˆæ— å¯è§†åŒ–æ¨¡å¼ï¼‰
            if create_gif and episode % snapshot_interval == 0:
                v_table = agent.get_v_table()
                policy = agent.get_policy()
                max_q_change = agent.get_max_q_change()
                avg_reward = np.mean(list(episode_rewards)[-100:]) if len(episode_rewards) >= 100 else np.mean(list(episode_rewards))
                
                stats = {
                    'episode': episode,
                    'max_episodes': max_episodes,
                    'epsilon': agent.epsilon,
                    'total_reward': total_reward,
                    'avg_reward': avg_reward,
                    'max_q_change': max_q_change,
                    'steps': steps
                }
                
                frame_path = os.path.join(frames_dir, f"frame_{episode:04d}.png")
                save_q_table_snapshot(agent.q_table, v_table, policy, env, stats, frame_path)
            
            # æ£€æŸ¥æ”¶æ•›æ¡ä»¶
            if episode > 50:
                converged, reason = check_convergence(agent, episode_rewards, train_config)
                if converged:
                    convergence_count += 1
                    if convergence_count >= convergence_patience:
                        console.print(f"\nğŸ‰ Training converged after {episode} episodes!", style="bold green")
                        console.print(f"   Reason: {reason}")
                        break
                else:
                    convergence_count = 0
    
    # è®­ç»ƒå®Œæˆç»Ÿè®¡
    console.print(f"\nâœ… Training completed!")
    console.print(f"   Total episodes: {episode}")
    console.print(f"   Final epsilon: {agent.epsilon:.6f}")
    
    if len(episode_rewards) >= 100:
        final_avg = np.mean(list(episode_rewards)[-100:])
        console.print(f"   Final 100-episode average reward: {final_avg:.4f}")
    
    # æ˜¾ç¤ºæœ€ç»ˆç­–ç•¥
    console.print(f"\nğŸ—ºï¸  Final Policy (Best Actions):")
    policy = agent.get_policy()
    action_symbols = ['â†‘', 'â†“', 'â†', 'â†’']
    
    for row in range(env.grid_size):
        row_str = ""
        for col in range(env.grid_size):
            pos = (row, col)
            if pos == env.start_pos:
                row_str += " S "
            elif pos == env.goal_pos:
                row_str += " G "
            elif pos in env.holes:
                row_str += " # "
            else:
                state_idx = env.get_state_index(pos)
                action = policy[state_idx]
                row_str += f" {action_symbols[action]} "
        console.print(row_str)
    
    # åˆ›å»ºGIFåŠ¨ç”»
    if create_gif:
        console.print(f"\nğŸ¬ Creating GIF animation...")
        create_gif_from_frames(frames_dir, gif_output_path, fps=config.get('viz', {}).get('fps', 2))
        console.print(f"   GIF saved to: {gif_output_path}")
    
    console.print(f"\nğŸ Training session finished.", style="bold green")


if __name__ == "__main__":
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs("outputs/frames", exist_ok=True)
    
    main()
